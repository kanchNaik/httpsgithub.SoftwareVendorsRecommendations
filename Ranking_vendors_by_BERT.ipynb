{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### Mounting google drive"
      ],
      "metadata": {
        "id": "YpgmE3aVIIpv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g0Xw-ujeEhKh",
        "outputId": "0c367a2d-4039-42f4-ac01-143c0f56ca92"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "rxalLzqfDPYd"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Added to support TPU"
      ],
      "metadata": {
        "id": "i4EgWwd2INl4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check TPU availability\n",
        "def get_device():\n",
        "    if torch.cuda.is_available():\n",
        "        return torch.device(\"cuda\")\n",
        "    elif torch.backends.mps.is_available():\n",
        "        return torch.device(\"mps\")  # Apple Silicon\n",
        "    elif torch.xpu.is_available():\n",
        "        return torch.device(\"xpu\")  # Intel GPU\n",
        "    else:\n",
        "        return torch.device(\"cpu\")\n",
        "\n",
        "device = get_device()"
      ],
      "metadata": {
        "id": "vpufJHs_EGdm"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Loading data"
      ],
      "metadata": {
        "id": "rQnggtd_IRdy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Dataset\n",
        "def load_data(csv_path):\n",
        "    df = pd.read_csv(csv_path)\n",
        "    df = df[['product_name', 'main_category', 'Features', 'rating']].copy()\n",
        "    df.fillna('', inplace=True)\n",
        "    df['combined_features'] = df.apply(\n",
        "        lambda row: f\"Category: {row['main_category']} Features: {row['Features']}\", axis=1\n",
        "    )\n",
        "    return df"
      ],
      "metadata": {
        "id": "tUaKESggDSCL"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Getting distilBert tokenizer"
      ],
      "metadata": {
        "id": "y8glsPwNITcK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')"
      ],
      "metadata": {
        "id": "OJTlBrc6ENAb"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Pre-processing data for bert with attension mask"
      ],
      "metadata": {
        "id": "1g-fZZ4sIZYw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare Data for Training\n",
        "def prepare_data(df):\n",
        "    texts = df['combined_features'].tolist()\n",
        "    labels = torch.tensor(df['rating'].tolist(), dtype=torch.float32).to(device)\n",
        "    encodings = tokenizer(texts, truncation=True, padding='max_length', max_length=128, return_tensors='pt')\n",
        "    dataset = TensorDataset(encodings['input_ids'].to(device), encodings['attention_mask'].to(device), labels)\n",
        "    return dataset"
      ],
      "metadata": {
        "id": "3rtlLrh_EOQS"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Loading model"
      ],
      "metadata": {
        "id": "4RcYcjfAIfPU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Model\n",
        "def load_model():\n",
        "    model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=1)\n",
        "    model.to(device)\n",
        "    return model"
      ],
      "metadata": {
        "id": "2REhHnxfDYXw"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training model with 3 epochs and batch processing"
      ],
      "metadata": {
        "id": "q2XGkgcGIhIl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train Model\n",
        "def train_model(df, epochs=3, batch_size=16):\n",
        "    dataset = prepare_data(df)\n",
        "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "    model = load_model()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=2e-5)\n",
        "    loss_fn = nn.SmoothL1Loss()  # Better for ranking\n",
        "\n",
        "    model.train()\n",
        "    for epoch in range(epochs):\n",
        "        total_loss = 0\n",
        "        for batch in dataloader:\n",
        "            input_ids, attention_mask, labels = batch\n",
        "            optimizer.zero_grad()\n",
        "            output = model(input_ids, attention_mask, labels=labels.unsqueeze(1))\n",
        "            loss = loss_fn(output.logits, labels.unsqueeze(1))\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item()\n",
        "        print(f\"Epoch {epoch+1}, Loss: {total_loss / len(dataloader)}\")\n",
        "    return model"
      ],
      "metadata": {
        "id": "_YDdvAcJEWa2"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Function to predict similarity score"
      ],
      "metadata": {
        "id": "UNJhMKjDImvN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Predict Function\n",
        "def predict(model, query):\n",
        "    encoding = tokenizer(query, truncation=True, padding='max_length', max_length=128, return_tensors='pt')\n",
        "    encoding = {key: value.to(device) for key, value in encoding.items()}\n",
        "    with torch.no_grad():\n",
        "        output = model(encoding['input_ids'], encoding['attention_mask'])\n",
        "\n",
        "    score = output.logits.item()\n",
        "    return np.clip(score / 5.0, 0, 1)  # Normalize to 0-1 range"
      ],
      "metadata": {
        "id": "dhHkYD1ADbP0"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training model"
      ],
      "metadata": {
        "id": "JWSPK-3KIsuL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example Usage\n",
        "csv_path = '/content/drive/MyDrive/Pyramyd OA/G2 software product overview.csv'\n",
        "df = load_data(csv_path)\n",
        "model = train_model(df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4OGzVFVeDefp",
        "outputId": "1ba00070-0e6f-4b1a-9fb8-b8361cac7c97"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss: 1.0038079870125605\n",
            "Epoch 2, Loss: 0.07223355487757732\n",
            "Epoch 3, Loss: 0.07472688893950175\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Searching vendors based on similarity"
      ],
      "metadata": {
        "id": "EzWc5NTaIviB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Search Vendors\n",
        "def search_vendors(model, df, category, feature_query, similarity_threshold=0.5):\n",
        "    query = f\"Category: {category} Features: {feature_query}\"\n",
        "\n",
        "    # Filter category first\n",
        "    category_mask = df['main_category'].str.lower().str.contains(category.lower(), na=False)\n",
        "    filtered_df = df[category_mask].copy()\n",
        "\n",
        "    if filtered_df.empty:\n",
        "        print(f\"No vendors found for category: {category}\")\n",
        "        return pd.DataFrame()\n",
        "\n",
        "    # Predict similarity for each vendor\n",
        "    filtered_df['similarity_score'] = filtered_df['combined_features'].apply(lambda text: predict(model, text))\n",
        "\n",
        "    # Filter by similarity threshold\n",
        "    matched_vendors = filtered_df[filtered_df['similarity_score'] >= similarity_threshold].copy()\n",
        "\n",
        "    if matched_vendors.empty:\n",
        "        print(\"No relevant vendors found based on the feature query.\")\n",
        "        return pd.DataFrame()\n",
        "\n",
        "    # Rank vendors by similarity and rating\n",
        "    matched_vendors['rating_normalized'] = matched_vendors['rating'] / 5.0\n",
        "    matched_vendors['final_score'] = (0.7 * matched_vendors['similarity_score'] +\n",
        "                                      0.3 * matched_vendors['rating_normalized'])\n",
        "\n",
        "    # Sort by ranking score\n",
        "    ranked_vendors = matched_vendors.sort_values('final_score', ascending=False).reset_index(drop=True)\n",
        "    return ranked_vendors[['product_name', 'main_category', 'rating', 'similarity_score', 'final_score']]"
      ],
      "metadata": {
        "id": "ixtfJDWNIASX"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Ranking vendors and getting top 10 vendors"
      ],
      "metadata": {
        "id": "1PHfiWlNIzc1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "category = \"Accounting & Finance Software\"\n",
        "feature_query = \"Budgeting\"\n",
        "matched_vendors_df = search_vendors(model, df, category, feature_query)\n",
        "\n",
        "# Display top results\n",
        "print(matched_vendors_df.head(10))"
      ],
      "metadata": {
        "id": "PbEJsneQIFLw"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
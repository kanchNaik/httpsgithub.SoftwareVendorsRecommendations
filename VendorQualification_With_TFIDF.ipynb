{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Vendor Qualification System**"
      ],
      "metadata": {
        "id": "FT4J6y8B7Kdl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Mounting Google Drive for data connection"
      ],
      "metadata": {
        "id": "oBA9quK_7Fiz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "62I_86fE5XFH",
        "outputId": "056d4b3e-52ce-44ce-d3ee-50154815bf0d"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eLpJznGp4eVj",
        "outputId": "8c2f20fa-6cfa-46cf-ed2c-7219431dc612"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n"
          ]
        }
      ],
      "source": [
        "pip install nltk"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import pandas as pd\n",
        "import json\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer, PorterStemmer"
      ],
      "metadata": {
        "id": "Czox56af5K2_"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Loading stopwords and wordnet for text pre-processing"
      ],
      "metadata": {
        "id": "LkZo-kYX7SjY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6XFaiyJL5OaA",
        "outputId": "40622a3d-7aa0-49e4-b10e-48bcb3941954"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lemmatizer = WordNetLemmatizer()\n",
        "stemmer = PorterStemmer()\n",
        "stop_words = set(stopwords.words('english'))"
      ],
      "metadata": {
        "id": "CF_5y3aH5T8_"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Loading data in pandas DataFrame"
      ],
      "metadata": {
        "id": "Xnz9ERZ87Ytm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the file path\n",
        "file_path = \"/content/drive/MyDrive/Pyramyd OA/G2 software product overview.csv\""
      ],
      "metadata": {
        "id": "rN2MtWFk5UyZ"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_data(file_path):\n",
        "    \"\"\"\n",
        "    Loads a CSV file into a Pandas DataFrame.\n",
        "\n",
        "    Args:\n",
        "        file_path (str): The path to the CSV file to be loaded.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: A DataFrame containing the data from the CSV file.\n",
        "    \"\"\"\n",
        "    df = pd.read_csv(file_path)\n",
        "    return df"
      ],
      "metadata": {
        "id": "S9r44AMS5eWq"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data info to check all attributes"
      ],
      "metadata": {
        "id": "mjXJo6bs7dxT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = load_data(file_path)\n",
        "df.info()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XK6DTzmr5fCK",
        "outputId": "d397e0a4-ba29-4777-887f-5332b41fe35f"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 1000 entries, 0 to 999\n",
            "Data columns (total 45 columns):\n",
            " #   Column                        Non-Null Count  Dtype  \n",
            "---  ------                        --------------  -----  \n",
            " 0   url                           1000 non-null   object \n",
            " 1   product_name                  1000 non-null   object \n",
            " 2   rating                        1000 non-null   float64\n",
            " 3   description                   996 non-null    object \n",
            " 4   product_url                   1000 non-null   object \n",
            " 5   seller                        1000 non-null   object \n",
            " 6   ownership                     230 non-null    object \n",
            " 7   seller_website                1000 non-null   object \n",
            " 8   headquarters                  1000 non-null   object \n",
            " 9   total_revenue                 169 non-null    object \n",
            " 10  social_media_profiles         1000 non-null   object \n",
            " 11  seller_description            1000 non-null   object \n",
            " 12  reviews_count                 1000 non-null   int64  \n",
            " 13  discussions_count             958 non-null    float64\n",
            " 14  pros_list                     998 non-null    object \n",
            " 15  cons_list                     998 non-null    object \n",
            " 16  competitors                   972 non-null    object \n",
            " 17  highest_rated_features        0 non-null      float64\n",
            " 18  lowest_rated_features         0 non-null      float64\n",
            " 19  rating_split                  1000 non-null   object \n",
            " 20  pricing                       943 non-null    object \n",
            " 21  official_screenshots          0 non-null      float64\n",
            " 22  official_downloads            793 non-null    object \n",
            " 23  official_videos               0 non-null      float64\n",
            " 24  categories                    1000 non-null   object \n",
            " 25  user_ratings                  0 non-null      float64\n",
            " 26  languages_supported           950 non-null    object \n",
            " 27  year_founded                  988 non-null    float64\n",
            " 28  position_against_competitors  839 non-null    object \n",
            " 29  overview                      1000 non-null   object \n",
            " 30  claimed                       1000 non-null   bool   \n",
            " 31  logo                          1000 non-null   object \n",
            " 32  reviews                       1000 non-null   object \n",
            " 33  top_alternatives              1000 non-null   object \n",
            " 34  top_alternatives_url          1000 non-null   object \n",
            " 35  full_pricing_page             943 non-null    object \n",
            " 36  badge                         919 non-null    object \n",
            " 37  what_is_description           0 non-null      float64\n",
            " 38  main_category                 1000 non-null   object \n",
            " 39  main_subject                  1000 non-null   object \n",
            " 40  Features                      975 non-null    object \n",
            " 41  region                        231 non-null    object \n",
            " 42  country_code                  1000 non-null   object \n",
            " 43  software_product_id           1000 non-null   object \n",
            " 44  overview_provided_by          1000 non-null   object \n",
            "dtypes: bool(1), float64(9), int64(1), object(34)\n",
            "memory usage: 344.9+ KB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Defining user query"
      ],
      "metadata": {
        "id": "Ww-c_2no7hyc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "software_category = \"Accounting & Finance Software\"\n",
        "capabilities = [\"Budgeting\"]"
      ],
      "metadata": {
        "id": "BJY8uov362H3"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Function to pre-process text"
      ],
      "metadata": {
        "id": "bJ94xi3s7kYG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_text(stemmer, lemmatizer, stop_words, text):\n",
        "    \"\"\"\n",
        "    Preprocesses the input text by:\n",
        "    - Converting to lowercase.\n",
        "    - Removing non-alphanumeric characters.\n",
        "    - Tokenizing the text into words.\n",
        "    - Removing stopwords.\n",
        "    - Lemmatizing and stemming the words.\n",
        "\n",
        "    Args:\n",
        "        stemmer (PorterStemmer): The stemmer to apply to each word.\n",
        "        lemmatizer (WordNetLemmatizer): The lemmatizer to apply to each word.\n",
        "        stop_words (set): Set of stopwords to be removed from the text.\n",
        "        text (str): The input text to be processed.\n",
        "\n",
        "    Returns:\n",
        "        str: The processed text as a single string, where each word is lemmatized, stemmed, and non-stopword.\n",
        "    \"\"\"\n",
        "    # Lowercase the text\n",
        "    text = text.lower()\n",
        "\n",
        "    # Remove special characters (non-alphanumeric characters)\n",
        "    text = re.sub(r'[^a-zA-Z0-9\\s]', '', text)\n",
        "\n",
        "    # Tokenization (split the text into words)\n",
        "    words = text.split()\n",
        "\n",
        "    # Remove stopwords and apply stemming and lemmatization\n",
        "    processed_words = []\n",
        "    for word in words:\n",
        "        if word not in stop_words:\n",
        "            # Lemmatize and stem\n",
        "            lemmatized_word = lemmatizer.lemmatize(word)\n",
        "            stemmed_word = stemmer.stem(lemmatized_word)\n",
        "            processed_words.append(stemmed_word)\n",
        "\n",
        "    # Join the processed words back into a string\n",
        "    return ' '.join(processed_words)\n",
        "\n",
        "def clean_json_for_csv(json_data):\n",
        "    \"\"\"\n",
        "    Cleans up JSON data by removing newline and carriage return characters to ensure proper CSV formatting.\n",
        "\n",
        "    Args:\n",
        "        json_data (str): The JSON data to be cleaned.\n",
        "\n",
        "    Returns:\n",
        "        str: The cleaned JSON data.\n",
        "    \"\"\"\n",
        "    return json_data.replace('\\n', ' ').replace('\\r', ' ')\n",
        "\n",
        "def get_query(software_category, capabilities):\n",
        "    \"\"\"\n",
        "    Constructs a query string based on the provided software category and capabilities.\n",
        "\n",
        "    If capabilities are provided, they are included in the query string. If only one capability is provided,\n",
        "    it will be added directly; otherwise, the capabilities will be joined with commas and 'and' for the last one.\n",
        "\n",
        "    Args:\n",
        "        software_category (str): The category of software.\n",
        "        capabilities (list): A list of capabilities to filter by.\n",
        "\n",
        "    Returns:\n",
        "        str: A query string describing the software category and its capabilities (if any).\n",
        "    \"\"\"\n",
        "    if capabilities:\n",
        "        if len(capabilities) == 1:\n",
        "            cap_str = capabilities[0]\n",
        "        else:\n",
        "            cap_str = ', '.join(capabilities[:-1]) + ' and ' + capabilities[-1]\n",
        "        query = f\"{software_category} with {cap_str}\"\n",
        "    else:\n",
        "        query = software_category\n",
        "\n",
        "    return query"
      ],
      "metadata": {
        "id": "leLoW5uA54H7"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Function to generate TF-IDF vectors"
      ],
      "metadata": {
        "id": "XkDrXAto7pv7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_tfidf_per_row(df_tfidf, directory_path=None):\n",
        "    \"\"\"\n",
        "    Generates TF-IDF vectors for each row in the DataFrame based on text features\n",
        "    provided in a JSON-formatted 'Features' column.\n",
        "\n",
        "    Args:\n",
        "        df_tfidf (pd.DataFrame): DataFrame containing a 'Features' column with JSON strings\n",
        "            describing various features and associated text data.\n",
        "        directory_path (str, optional): Unused parameter in current implementation. Reserved for future use.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: The updated DataFrame with two new columns:\n",
        "            - 'vectors': A dictionary of TF-IDF vectors (as lists) per feature.\n",
        "            - 'vectorizers': A dictionary of fitted TfidfVectorizer objects per feature.\n",
        "\n",
        "    Notes:\n",
        "        - The function preprocesses text using lemmatization, stemming, and stop word removal.\n",
        "        - It handles rows with invalid or empty JSON gracefully and logs errors.\n",
        "        - Each feature's TF-IDF vector is generated using a separate `TfidfVectorizer` instance.\n",
        "    \"\"\"\n",
        "\n",
        "    # Initialize new columns to store vectors and vectorizers\n",
        "    df_tfidf['vectors'] = None\n",
        "    df_tfidf['vectorizers'] = None\n",
        "\n",
        "    for idx, row in df_tfidf.iterrows():\n",
        "        vectors = {}       # To store actual TF-IDF vectors\n",
        "        vectorizers = {}   # To store actual TfidfVectorizer objects\n",
        "        column_value_as_string = str(row['Features'])\n",
        "\n",
        "        # Check if the string is empty or just whitespace\n",
        "        if column_value_as_string.strip():\n",
        "            try:\n",
        "                featuresValues = json.loads(column_value_as_string)\n",
        "\n",
        "                # Iterate over features and generate TF-IDF vectors\n",
        "                for categories in featuresValues:\n",
        "                    for feature in categories.get(\"features\", []):\n",
        "                        name = feature.get('name', '')\n",
        "                        description = feature.get('description', '')\n",
        "                        description += ' ' + name + ' ' + categories.get('Category', '') + ' ' + row['main_category']\n",
        "\n",
        "                        if description.strip():\n",
        "                            processed_description = preprocess_text(stemmer, lemmatizer, stop_words, description)\n",
        "                            vectorizer = TfidfVectorizer()\n",
        "                            tfidf_matrix = vectorizer.fit_transform([processed_description])\n",
        "\n",
        "                            # Store the vector and the vectorizer object\n",
        "                            vectors[name] = tfidf_matrix.toarray().tolist()\n",
        "                            vectorizers[name] = vectorizer\n",
        "                        else:\n",
        "                            vectors[name] = None\n",
        "                            vectorizers[name] = None\n",
        "\n",
        "                # Store in DataFrame\n",
        "                df_tfidf.at[idx, 'vectors'] = vectors\n",
        "                df_tfidf.at[idx, 'vectorizers'] = vectorizers\n",
        "\n",
        "            except json.JSONDecodeError as e:\n",
        "                print(f\"Error decoding JSON for row {idx}: {e}\")\n",
        "                df_tfidf.at[idx, 'vectors'] = None\n",
        "                df_tfidf.at[idx, 'vectorizers'] = None\n",
        "        else:\n",
        "            print(f\"Empty or invalid JSON for row {idx}\")\n",
        "            df_tfidf.at[idx, 'vectors'] = None\n",
        "            df_tfidf.at[idx, 'vectorizers'] = None\n",
        "\n",
        "    return df_tfidf"
      ],
      "metadata": {
        "id": "PcoOgEG45u67"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Function to calculate similarity"
      ],
      "metadata": {
        "id": "rEZAjZpf7up7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_similarity(query, df):\n",
        "    \"\"\"\n",
        "    Calculates similarity scores between a preprocessed user query and text-based feature vectors\n",
        "    stored in a DataFrame for each row.\n",
        "\n",
        "    Args:\n",
        "        query (str): The input query string to compare against feature vectors.\n",
        "        df (pd.DataFrame): A DataFrame containing the following columns:\n",
        "            - 'vectors': A dictionary of precomputed feature vectors for each row.\n",
        "            - 'vectorizers': A dictionary of fitted vectorizer objects corresponding to each feature.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: The original DataFrame with the following added columns:\n",
        "            - 'similarity_scores': A dictionary of cosine similarity scores per feature.\n",
        "            - 'avg_similarity_scores': The average similarity score across all features in a row.\n",
        "\n",
        "    Notes:\n",
        "        - The query is first preprocessed using stemming, lemmatization, and stop word removal.\n",
        "        - Cosine similarity is used to compare the vectorized query with stored feature vectors.\n",
        "        - Handles missing vectorizers, unfitted vectorizers, and dimension mismatches.\n",
        "        - Drops 'vectors' and 'vectorizers' columns before returning the final DataFrame.\n",
        "    \"\"\"\n",
        "\n",
        "    processed_query = preprocess_text(stemmer, lemmatizer, stop_words, query)\n",
        "\n",
        "    similarity_results = []\n",
        "    avg_similarity_scores = []\n",
        "\n",
        "    for idx, row in df.iterrows():\n",
        "        similarity_scores = {}\n",
        "\n",
        "        feature_vectors = row.get('vectors', {})\n",
        "        feature_vectorizers = row.get('vectorizers', {})\n",
        "\n",
        "        if feature_vectors and feature_vectorizers:\n",
        "            total_score = 0\n",
        "            count = 0\n",
        "\n",
        "            for feature_name, vector in feature_vectors.items():\n",
        "                print(f\"Processing feature '{feature_name}' in row {idx}\")\n",
        "\n",
        "                if vector:\n",
        "                    try:\n",
        "                        vectorizer = feature_vectorizers.get(feature_name, None)\n",
        "\n",
        "                        if vectorizer is None:\n",
        "                            print(f\"No vectorizer for feature '{feature_name}' in row {idx}\")\n",
        "                            similarity_scores[feature_name] = 0\n",
        "                            continue\n",
        "\n",
        "                        if not hasattr(vectorizer, 'vocabulary_'):\n",
        "                            print(f\"Error: Vectorizer not fitted for feature '{feature_name}' in row {idx}\")\n",
        "                            similarity_scores[feature_name] = 0\n",
        "                            continue\n",
        "\n",
        "                        # Transform the query\n",
        "                        query_vector = vectorizer.transform([processed_query])\n",
        "                        print(f\"Query Vector Shape: {query_vector.shape}\")\n",
        "\n",
        "                        # Convert stored feature vector to numpy array\n",
        "                        feature_vector = np.array(vector).reshape(1, -1)\n",
        "\n",
        "                        if query_vector.shape[1] == feature_vector.shape[1]:\n",
        "                            similarity = cosine_similarity(query_vector, feature_vector)[0][0]\n",
        "                        else:\n",
        "                            print(f\"Dimension mismatch in row {idx}, feature '{feature_name}'\")\n",
        "                            similarity = 0\n",
        "\n",
        "                        total_score += similarity\n",
        "                        count += 1\n",
        "                        similarity_scores[feature_name] = similarity\n",
        "\n",
        "                    except Exception as e:\n",
        "                        print(f\"Error processing feature '{feature_name}' in row {idx}: {e}\")\n",
        "                        similarity_scores[feature_name] = 0\n",
        "                else:\n",
        "                    similarity_scores[feature_name] = 0\n",
        "\n",
        "            #avg similarity scores calculations\n",
        "            avg_similarity_scores.append(total_score / count if count > 0 else 0)\n",
        "            similarity_results.append(similarity_scores)\n",
        "        else:\n",
        "            avg_similarity_scores.append(0)\n",
        "            similarity_results.append({})\n",
        "\n",
        "    #droping vectors once their work is done\n",
        "    df.drop(['vectors', 'vectorizers'], axis=1, inplace=True)\n",
        "\n",
        "    df['similarity_scores'] = similarity_results\n",
        "    df['avg_similarity_scores'] = avg_similarity_scores\n",
        "\n",
        "    return df"
      ],
      "metadata": {
        "id": "JVY0E2GQ6YqG"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Function to filter DFs where atleast one feature has 0.6 threshold"
      ],
      "metadata": {
        "id": "zUHcvad37yiG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def filter_highly_similar_rows(df, threshold=0.6):\n",
        "    \"\"\"\n",
        "    Filters rows in the DataFrame where at least one feature's similarity score\n",
        "    is greater than or equal to the given threshold.\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): The input DataFrame containing a 'similarity_scores' column\n",
        "            (a dictionary of similarity scores per feature) and 'avg_similarity_scores'.\n",
        "        threshold (float, optional): The minimum similarity score to consider a row relevant.\n",
        "            Defaults to 0.6.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: A filtered DataFrame containing only the rows with at least one\n",
        "        feature whose similarity score meets or exceeds the threshold, sorted by\n",
        "        'avg_similarity_scores' in descending order.\n",
        "    \"\"\"\n",
        "\n",
        "    # Function to check if any feature in a row has similarity >= threshold\n",
        "    def has_high_similarity(similarity_scores):\n",
        "        return any(score >= threshold for score in similarity_scores.values())\n",
        "\n",
        "    # Filter the DataFrame\n",
        "    filtered_df = df[df['similarity_scores'].apply(has_high_similarity)]\n",
        "\n",
        "    return filtered_df.sort_values(by='avg_similarity_scores', ascending=False)"
      ],
      "metadata": {
        "id": "tUTvbxKI6ief"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Function to rank all vendors"
      ],
      "metadata": {
        "id": "TNiHPUH574ph"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def rank_vendors(df, weight_similarity=0.7, weight_rating=0.3):\n",
        "    \"\"\"\n",
        "    Ranks vendors based on weighted average of similarity and rating.\n",
        "\n",
        "    Parameters:\n",
        "    - df: DataFrame with columns `avg_similarity_scores`, `rating`, and optionally `prequalified`\n",
        "    - weight_similarity: weight for average similarity score (default 0.7)\n",
        "    - weight_rating: weight for vendor rating (default 0.3)\n",
        "    - top_n: number of top vendors to return (default 10)\n",
        "\n",
        "    Returns:\n",
        "    - Ranked DataFrame with score and rank\n",
        "    \"\"\"\n",
        "    df_ranked = df.copy()\n",
        "\n",
        "    # Fill missing ratings with 0 if any\n",
        "    if 'rating' in df_ranked.columns:\n",
        "        df_ranked['rating'] = df_ranked['rating'].fillna(0)\n",
        "    else:\n",
        "        df_ranked['rating'] = 0  # Add rating column if not present\n",
        "\n",
        "    # Normalize both columns to bring them into [0,1] range\n",
        "    df_ranked['normalized_similarity'] = df_ranked['avg_similarity_scores'] / df_ranked['avg_similarity_scores'].max()\n",
        "    df_ranked['normalized_rating'] = df_ranked['rating'] / df_ranked['rating'].max() if df_ranked['rating'].max() > 0 else 0\n",
        "\n",
        "    # Compute final score using weighted sum\n",
        "    df_ranked['final_score'] = (\n",
        "        weight_similarity * df_ranked['normalized_similarity'] +\n",
        "        weight_rating * df_ranked['normalized_rating']\n",
        "    )\n",
        "\n",
        "\n",
        "    # Sort: Prequalified vendors first, then by final score\n",
        "    df_ranked.sort_values(by=['final_score'], ascending=[False], inplace=True)\n",
        "    df_ranked['rank'] = range(1, len(df_ranked) + 1)\n",
        "\n",
        "    return df_ranked"
      ],
      "metadata": {
        "id": "LdHotigD6tbu"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Function to return ranked vendors"
      ],
      "metadata": {
        "id": "4PBE4b6m78hV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_qualifiedVendors(input_path, query, software_category, capabilities):\n",
        "    \"\"\"\n",
        "    Filters and ranks vendors based on similarity to a query, within a specified software category,\n",
        "    using TF-IDF vectorization and cosine similarity. The function returns the top 10 vendors based on\n",
        "    their similarity scores and ranking.\n",
        "\n",
        "    Args:\n",
        "        input_path (str): Path to the input data file containing vendor information.\n",
        "        query (str): The query text to compare against the vendors' features for similarity.\n",
        "        software_category (str): The software category to filter the vendors by (case-insensitive).\n",
        "        capabilities (dict): Additional capabilities or filters (not used in this implementation but reserved for future extensions).\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: A DataFrame containing the top 10 vendors sorted by their similarity to the query, including:\n",
        "            - 'product_name': Name of the product/vendor.\n",
        "            - 'rating': Vendor's rating.\n",
        "            - 'seller': Vendor's seller.\n",
        "            - 'main_category': Vendor's main software category.\n",
        "            - 'Features': Features associated with the vendor.\n",
        "            - 'avg_similarity_scores': The average similarity score of the vendor's features to the query.\n",
        "            - 'final_score': Final score after ranking.\n",
        "            - 'rank': Rank based on the final score.\n",
        "\n",
        "    Notes:\n",
        "        - The function preprocesses text and calculates TF-IDF vectors for the vendor features.\n",
        "        - The vendors are filtered by their main category, then ranked based on their similarity to the input query.\n",
        "        - The function assumes the input data is in a compatible format (e.g., CSV or JSON).\n",
        "    \"\"\"\n",
        "\n",
        "    # Load vendor data from the specified file path\n",
        "    df = load_data(input_path)\n",
        "\n",
        "    # Select relevant columns from the data\n",
        "    df = df[['product_name', 'rating', 'seller', 'main_category', 'Features']]\n",
        "\n",
        "    # Filter vendors by the specified software category (case-insensitive)\n",
        "    df = df[df['main_category'].str.contains(software_category, case=False, na=False)]\n",
        "\n",
        "    # Generate TF-IDF vectors for each row based on the 'Features' column\n",
        "    df = generate_tfidf_per_row(df.copy())\n",
        "\n",
        "    # Calculate similarity scores between the query and vendor feature vectors\n",
        "    df_new = calculate_similarity(query, df.copy())\n",
        "\n",
        "    # Filter vendors that have highly similar features to the query (above a predefined threshold)\n",
        "    filtereddf = filter_highly_similar_rows(df_new)\n",
        "\n",
        "    # Rank the vendors based on their similarity scores\n",
        "    rankedvendors = rank_vendors(filtereddf)\n",
        "\n",
        "    # Return the top 10 vendors with the relevant information\n",
        "    return rankedvendors[['product_name', 'rating', 'seller', 'main_category', 'Features', 'avg_similarity_scores', 'final_score', 'rank']].head(10)\n"
      ],
      "metadata": {
        "id": "7Vpij7JV6rHL"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Return final output"
      ],
      "metadata": {
        "id": "TZF_Lq418FCB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def vendor_qualification ():\n",
        "\n",
        "    \"\"\"\n",
        "    Endpoint to qualify vendors based on similarity to a provided query and filter by software category and capabilities.\n",
        "    This function processes the incoming request, retrieves the list of qualified vendors, and returns them in a JSON format.\n",
        "\n",
        "    Request Arguments:\n",
        "        - software_category (str): The category of software the vendors must belong to.\n",
        "        - capabilities (list): A list of capabilities used to refine the query (though not utilized in the current implementation).\n",
        "\n",
        "    Returns:\n",
        "        JSON Response:\n",
        "            - 'message': A static message indicating the purpose of the endpoint ('Vendor Qualification').\n",
        "            - 'similarity_scores': A list of top 10 qualified vendors, including product name, rating, seller, features, and similarity scores.\n",
        "    \"\"\"\n",
        "\n",
        "\n",
        "\n",
        "    query = get_query(software_category, capabilities)\n",
        "\n",
        "    qualifiedVendors = get_qualifiedVendors(\"C:\\\\Users\\\\naikn\\\\Downloads\\\\G2 software product overview.csv\", query, software_category, capabilities)\n",
        "    return qualifiedVendors"
      ],
      "metadata": {
        "id": "45UemQV55h2-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}